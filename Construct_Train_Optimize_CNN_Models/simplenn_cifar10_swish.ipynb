{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tinayiluo0322/Computer-Engineering-Machine-Learning-and-Deep-Neural-Nets-Projects/blob/main/Construct_Train_Optimize_CNN_Models/simplenn_cifar10_swish.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FHU0uE10-zfR"
      },
      "source": [
        "# Optimizing SimpleNN on CIFAR-10\n",
        "\n",
        "#### Luopeiwen Yi\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AT2V-hEjUgCL",
        "outputId": "d3bee8f9-8ff0-4c13-c1f9-0ca1fcde37a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import os"
      ],
      "metadata": {
        "id": "6c_9k29BVqpd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Change this to the absolute path where dataset.py and utils.py are stored\n",
        "CODE_PATH = \"/content/drive/MyDrive/CNN_hw\"\n",
        "\n",
        "# Add this path to sys.path so Python can find it\n",
        "sys.path.append(CODE_PATH)\n",
        "\n",
        "# Check if Colab can see the files\n",
        "print(\"Files in directory:\", os.listdir(CODE_PATH))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y93THHD5Ul8F",
        "outputId": "be564b7e-c241-4201-f49f-2eb1ffeeadc2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files in directory: ['sample_predictions.csv', 'save_test_predictions.ipynb', '__pycache__', 'tools', 'simplenn-cifar10.ipynb']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5hTIizt-zfS"
      },
      "source": [
        "# Optimization 3: Activation Function\n",
        "\n",
        "## Step 1: Build the model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TwB63cqB-zfS"
      },
      "outputs": [],
      "source": [
        "# import necessary dependencies\n",
        "import argparse\n",
        "import os, sys\n",
        "import time\n",
        "import datetime\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Swish(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return x * torch.sigmoid(x)  # Swish function"
      ],
      "metadata": {
        "id": "ie492r7WOO5n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define SimpleNN with batch normalization\n",
        "class SimpleNN_BN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleNN_BN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 8, 5)\n",
        "        self.bn1 = nn.BatchNorm2d(8)  # BN Layer after conv1\n",
        "        self.conv2 = nn.Conv2d(8, 16, 3)\n",
        "        self.bn2 = nn.BatchNorm2d(16)  # BN Layer after conv2\n",
        "        self.fc1   = nn.Linear(16 * 6 * 6, 120)\n",
        "        self.fc2   = nn.Linear(120, 84)\n",
        "        self.fc3   = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))  # Apply BN before activation\n",
        "        out = F.max_pool2d(out, 2)\n",
        "        out = F.relu(self.bn2(self.conv2(out)))\n",
        "        out = F.max_pool2d(out, 2)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = F.relu(self.fc1(out))\n",
        "        out = F.relu(self.fc2(out))\n",
        "        out = self.fc3(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "jREA7TdlQDLd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LjnAHdxy-zfT"
      },
      "outputs": [],
      "source": [
        "class SimpleNN_BN_Swish(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleNN_BN_Swish, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 8, 5)\n",
        "        self.bn1 = nn.BatchNorm2d(8)\n",
        "        self.conv2 = nn.Conv2d(8, 16, 3)\n",
        "        self.bn2 = nn.BatchNorm2d(16)\n",
        "        self.fc1   = nn.Linear(16 * 6 * 6, 120)\n",
        "        self.fc2   = nn.Linear(120, 84)\n",
        "        self.fc3   = nn.Linear(84, 10)\n",
        "        self.swish = Swish()  # Swish activation\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.swish(self.bn1(self.conv1(x)))\n",
        "        out = F.max_pool2d(out, 2)\n",
        "        out = self.swish(self.bn2(self.conv2(out)))\n",
        "        out = F.max_pool2d(out, 2)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.swish(self.fc1(out))\n",
        "        out = self.swish(self.fc2(out))\n",
        "        out = self.fc3(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2dWCG-f-zfT"
      },
      "source": [
        "### Sanity Check"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the SimpleNN model\n",
        "model = SimpleNN_BN_Swish()\n",
        "\n",
        "# Create a dummy input tensor with the same shape as CIFAR-10 images (batch_size=1, channels=3, height=32, width=32)\n",
        "dummy_input = torch.randn(1, 3, 32, 32)  # Shape: (1, 3, 32, 32)\n",
        "\n",
        "# Pass the dummy input through the model\n",
        "output = model(dummy_input)\n",
        "\n",
        "# Check the output shape\n",
        "print(f\"Output shape: {output.shape}\")  # Should be (1, 10) since we have 10 classes\n",
        "\n",
        "# Count total number of parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Total number of parameters: {total_params}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fRsa5x3SMwRN",
        "outputId": "dba1b568-9f04-494b-f93d-b6c5f97eebc6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output shape: torch.Size([1, 10])\n",
            "Total number of parameters: 82078\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchsummary import summary\n",
        "\n",
        "# Set device to GPU if available, otherwise CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Define the SimpleNN model and move it to the selected device\n",
        "model = SimpleNN_BN_Swish().to(device)\n",
        "\n",
        "# Create a dummy input tensor and move it to the same device as the model\n",
        "dummy_input = torch.randn(1, 3, 32, 32).to(device)  # Ensure input is on the same device\n",
        "\n",
        "# Pass the dummy input through the model\n",
        "output = model(dummy_input)\n",
        "\n",
        "# Check the output shape\n",
        "print(f\"Output shape: {output.shape}\")  # Should be (1, 10) since we have 10 classes\n",
        "\n",
        "# Count total number of parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Total number of parameters: {total_params}\")\n",
        "\n",
        "# Print model summary (Ensure model is on the correct device)\n",
        "summary(model, (3, 32, 32), device=device.type)  # Specify the device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cNgO5dK7gbv7",
        "outputId": "6c81a043-41a0-45cc-afd2-f71979569a79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Output shape: torch.Size([1, 10])\n",
            "Total number of parameters: 82078\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1            [-1, 8, 28, 28]             608\n",
            "       BatchNorm2d-2            [-1, 8, 28, 28]              16\n",
            "             Swish-3            [-1, 8, 28, 28]               0\n",
            "            Conv2d-4           [-1, 16, 12, 12]           1,168\n",
            "       BatchNorm2d-5           [-1, 16, 12, 12]              32\n",
            "             Swish-6           [-1, 16, 12, 12]               0\n",
            "            Linear-7                  [-1, 120]          69,240\n",
            "             Swish-8                  [-1, 120]               0\n",
            "            Linear-9                   [-1, 84]          10,164\n",
            "            Swish-10                   [-1, 84]               0\n",
            "           Linear-11                   [-1, 10]             850\n",
            "================================================================\n",
            "Total params: 82,078\n",
            "Trainable params: 82,078\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 0.20\n",
            "Params size (MB): 0.31\n",
            "Estimated Total Size (MB): 0.52\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WX9zqVZk-zfT"
      },
      "source": [
        "## Step 1: Set up preprocessing functions\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Data augmentation for training set\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),  # Random cropping\n",
        "    transforms.RandomHorizontalFlip(),  # Random flipping\n",
        "    transforms.ToTensor(),  # Convert images to PyTorch tensors\n",
        "    transforms.Normalize(mean=(0.4914, 0.4822, 0.4465), std=(0.2023, 0.1994, 0.2010))  # Normalize\n",
        "])\n",
        "\n",
        "# No data augmentation for validation set\n",
        "transform_val = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=(0.4914, 0.4822, 0.4465), std=(0.2023, 0.1994, 0.2010))\n",
        "])"
      ],
      "metadata": {
        "id": "ydUJrZOwOcFz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ToTensor(): Converts PIL images to PyTorch tensors so they can be used in deep learning models.\n",
        "\n",
        "Normalize(mean, std): Standardizes pixel values to a mean of (0.4914, 0.4822, 0.4465) and std of (0.2023, 0.1994, 0.2010), helping the model converge faster.\n"
      ],
      "metadata": {
        "id": "MCNib8fBOoHv"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHl6i1gP-zfT"
      },
      "source": [
        "## Step 2: Set up dataset and dataloader\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Do NOT change these\n",
        "from tools.dataset import CIFAR10\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# A few arguments, do NOT change these\n",
        "DATA_ROOT = \"./data\"\n",
        "TRAIN_BATCH_SIZE = 128\n",
        "VAL_BATCH_SIZE = 100\n",
        "\n",
        "# Construct dataset\n",
        "train_set = CIFAR10(\n",
        "    root=DATA_ROOT,\n",
        "    mode='train',\n",
        "    download=True,\n",
        "    transform=transform_train  # Apply training preprocessing\n",
        ")\n",
        "\n",
        "val_set = CIFAR10(\n",
        "    root=DATA_ROOT,\n",
        "    mode='val',\n",
        "    download=True,\n",
        "    transform=transform_val  # Apply validation preprocessing\n",
        ")\n",
        "\n",
        "# Construct dataloaders\n",
        "train_loader = DataLoader(\n",
        "    train_set,\n",
        "    batch_size=TRAIN_BATCH_SIZE,  # Use predefined batch size\n",
        "    shuffle=True,  # Shuffle training data for randomness\n",
        "    num_workers=4  # Speed up data loading\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_set,\n",
        "    batch_size=VAL_BATCH_SIZE,  # Use predefined batch size\n",
        "    shuffle=False,  # No need to shuffle validation data\n",
        "    num_workers=4\n",
        ")\n",
        "\n",
        "# Sanity Check: Print dataset sizes\n",
        "print(f\"Train dataset size: {len(train_set)} images\")\n",
        "print(f\"Validation dataset size: {len(val_set)} images\")\n",
        "\n",
        "# Check a single batch\n",
        "sample_batch, sample_labels = next(iter(train_loader))\n",
        "print(f\"Sample batch shape: {sample_batch.shape}, Labels shape: {sample_labels.shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ElpvReM0QMta",
        "outputId": "1f47260d-3eab-4042-ab96-ff1b9c704b2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.dropbox.com/s/s8orza214q45b23/cifar10_trainval_F22.zip?dl=1 to ./data/cifar10_trainval_F22.zip\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "141746176it [00:03, 46762805.73it/s]                                \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar10_trainval_F22.zip to ./data\n",
            "Files already downloaded and verified\n",
            "Using downloaded and verified file: ./data/cifar10_trainval_F22.zip\n",
            "Extracting ./data/cifar10_trainval_F22.zip to ./data\n",
            "Files already downloaded and verified\n",
            "Train dataset size: 45000 images\n",
            "Validation dataset size: 5000 images\n",
            "Sample batch shape: torch.Size([128, 3, 32, 32]), Labels shape: torch.Size([128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Load one batch of training data\n",
        "data_iter = iter(train_loader)\n",
        "images, labels = next(data_iter)\n",
        "\n",
        "# Function to unnormalize and display images\n",
        "def imshow(img):\n",
        "    img = img.numpy().transpose((1, 2, 0))  # Convert to HWC format\n",
        "    mean = np.array([0.4914, 0.4822, 0.4465])\n",
        "    std = np.array([0.2023, 0.1994, 0.2010])\n",
        "    img = img * std + mean  # Unnormalize\n",
        "    img = np.clip(img, 0, 1)\n",
        "    plt.imshow(img)\n",
        "\n",
        "# Plot some augmented images\n",
        "fig, axes = plt.subplots(1, 5, figsize=(15,3))\n",
        "for i in range(5):\n",
        "    imshow(images[i])\n",
        "    axes[i].axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        },
        "id": "9koobFWv6ekY",
        "outputId": "16b8b1f9-9170-49fb-ad7f-70f744b55926"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1500x300 with 5 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WknGJMp1-zfU"
      },
      "source": [
        "## Step 4: Set up the loss function and optimizer\n",
        "Loss function/objective function is used to provide \"feedback\" for the neural networks. Typically, we use multi-class cross-entropy as the loss function for classification models. As for the optimizer, we will use SGD with momentum."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Hyperparameters (Do NOT change)\n",
        "INITIAL_LR = 0.1  # Initial learning rate\n",
        "MOMENTUM = 0.9  # Momentum for optimizer\n",
        "REG = 1e-4  # L2 regularization (weight decay)\n",
        "\n",
        "# Create loss function (Cross-Entropy Loss)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Add optimizer (SGD with Momentum and L2 Regularization)\n",
        "optimizer = optim.SGD(\n",
        "    model.parameters(),  # Optimizing model parameters\n",
        "    lr=INITIAL_LR,  # Learning rate\n",
        "    momentum=MOMENTUM,  # Momentum factor\n",
        "    weight_decay=REG  # L2 regularization\n",
        ")\n",
        "\n",
        "# Sanity check: Print optimizer details\n",
        "print(optimizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n7qWQMaSie7z",
        "outputId": "dfa1a7c4-971b-4c09-9851-34f4882f0920"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SGD (\n",
            "Parameter Group 0\n",
            "    dampening: 0\n",
            "    differentiable: False\n",
            "    foreach: None\n",
            "    fused: None\n",
            "    lr: 0.1\n",
            "    maximize: False\n",
            "    momentum: 0.9\n",
            "    nesterov: False\n",
            "    weight_decay: 0.0001\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13GUGlfU-zfU"
      },
      "source": [
        "## Step 5: Start the training process.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "\n",
        "def train_model(model, optimizer, filename, epochs=30):\n",
        "    \"\"\" Train the model and save the best checkpoint \"\"\"\n",
        "\n",
        "    CHECKPOINT_FOLDER = \"./saved_model_dev\"\n",
        "    best_val_acc = 0\n",
        "\n",
        "    # Ensure model is on the correct device\n",
        "    model.to(device)\n",
        "\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    initial_loss = 0\n",
        "    total_examples = 0\n",
        "    correct_examples = 0\n",
        "\n",
        "    with torch.no_grad():  # No gradient calculation\n",
        "        for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "            # Copy inputs and targets to the device\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "            # Forward pass: compute the output\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = criterion(outputs, targets)\n",
        "            initial_loss += loss.item()\n",
        "\n",
        "            # Compute accuracy before training\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct_examples += (predicted == targets).sum().item()\n",
        "            total_examples += targets.size(0)\n",
        "\n",
        "            # Only calculate on a small subset (e.g., 1 batch)\n",
        "            if batch_idx == 0:\n",
        "                break\n",
        "\n",
        "    # Compute initial average loss and accuracy\n",
        "    initial_loss /= (batch_idx + 1)\n",
        "    initial_acc = correct_examples / total_examples\n",
        "    print(f\"Initial loss before training: {initial_loss:.4f}, Initial accuracy: {initial_acc:.4f}\")\n",
        "\n",
        "\n",
        "    print(f\"==> Training {filename} model\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    for i in range(epochs):\n",
        "        model.train()  # Set model to training mode\n",
        "        print(f\"Epoch {i}:\")\n",
        "\n",
        "        total_examples = 0\n",
        "        correct_examples = 0\n",
        "        train_loss = 0\n",
        "\n",
        "        # Train loop\n",
        "        for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            # Zero gradients & backpropagation\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct_examples += (predicted == targets).sum().item()\n",
        "            total_examples += targets.size(0)\n",
        "\n",
        "        avg_loss = train_loss / len(train_loader)\n",
        "        avg_acc = correct_examples / total_examples\n",
        "        print(f\"Training loss: {avg_loss:.4f}, Training accuracy: {avg_acc:.4f}\")\n",
        "\n",
        "        # Validation loop\n",
        "        model.eval()\n",
        "        total_examples = 0\n",
        "        correct_examples = 0\n",
        "        val_loss = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch_idx, (inputs, targets) in enumerate(val_loader):\n",
        "                inputs, targets = inputs.to(device), targets.to(device)\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, targets)\n",
        "                val_loss += loss.item()\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                correct_examples += (predicted == targets).sum().item()\n",
        "                total_examples += targets.size(0)\n",
        "\n",
        "        avg_loss = val_loss / len(val_loader)\n",
        "        avg_acc = correct_examples / total_examples\n",
        "        print(f\"Validation loss: {avg_loss:.4f}, Validation accuracy: {avg_acc:.4f}\")\n",
        "\n",
        "        # Save best model\n",
        "        if avg_acc > best_val_acc:\n",
        "            best_val_acc = avg_acc\n",
        "            if not os.path.exists(CHECKPOINT_FOLDER):\n",
        "                os.makedirs(CHECKPOINT_FOLDER)\n",
        "            print(f\"Saving best model for {filename}...\")\n",
        "            torch.save({'state_dict': model.state_dict()}, os.path.join(CHECKPOINT_FOLDER, filename))\n",
        "\n",
        "        print('')\n",
        "\n",
        "    print(\"=\" * 50)\n",
        "    print(f\"==> Training finished for {filename}! Best validation accuracy: {best_val_acc:.4f}\")"
      ],
      "metadata": {
        "id": "ItUnWRx2kfzR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Comparison"
      ],
      "metadata": {
        "id": "PNtlkcSSQeLe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train SimpleNN with BN & ReLU\n",
        "model_bn_relu = SimpleNN_BN().to(device)\n",
        "optimizer_bn_relu = torch.optim.SGD(model_bn_relu.parameters(), lr=0.1, momentum=0.9, weight_decay=1e-4)\n",
        "train_model(model_bn_relu, optimizer_bn_relu, \"model_bn_relu.pth\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nzu0JXHUQfYD",
        "outputId": "64fd2347-0a49-4cef-a43f-5f65d97b3439"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial loss before training: 2.3165, Initial accuracy: 0.0938\n",
            "==> Training model_bn_relu.pth model\n",
            "==================================================\n",
            "Epoch 0:\n",
            "Training loss: 1.7022, Training accuracy: 0.3704\n",
            "Validation loss: 1.4235, Validation accuracy: 0.4786\n",
            "Saving best model for model_bn_relu.pth...\n",
            "\n",
            "Epoch 1:\n",
            "Training loss: 1.4617, Training accuracy: 0.4682\n",
            "Validation loss: 1.3142, Validation accuracy: 0.5208\n",
            "Saving best model for model_bn_relu.pth...\n",
            "\n",
            "Epoch 2:\n",
            "Training loss: 1.3883, Training accuracy: 0.5037\n",
            "Validation loss: 1.2565, Validation accuracy: 0.5490\n",
            "Saving best model for model_bn_relu.pth...\n",
            "\n",
            "Epoch 3:\n",
            "Training loss: 1.3083, Training accuracy: 0.5355\n",
            "Validation loss: 1.1890, Validation accuracy: 0.5780\n",
            "Saving best model for model_bn_relu.pth...\n",
            "\n",
            "Epoch 4:\n",
            "Training loss: 1.2577, Training accuracy: 0.5563\n",
            "Validation loss: 1.1799, Validation accuracy: 0.5762\n",
            "\n",
            "Epoch 5:\n",
            "Training loss: 1.2126, Training accuracy: 0.5734\n",
            "Validation loss: 1.1811, Validation accuracy: 0.5824\n",
            "Saving best model for model_bn_relu.pth...\n",
            "\n",
            "Epoch 6:\n",
            "Training loss: 1.1983, Training accuracy: 0.5765\n",
            "Validation loss: 1.0923, Validation accuracy: 0.6130\n",
            "Saving best model for model_bn_relu.pth...\n",
            "\n",
            "Epoch 7:\n",
            "Training loss: 1.1686, Training accuracy: 0.5896\n",
            "Validation loss: 1.1427, Validation accuracy: 0.6066\n",
            "\n",
            "Epoch 8:\n",
            "Training loss: 1.1550, Training accuracy: 0.5960\n",
            "Validation loss: 1.0496, Validation accuracy: 0.6326\n",
            "Saving best model for model_bn_relu.pth...\n",
            "\n",
            "Epoch 9:\n",
            "Training loss: 1.1341, Training accuracy: 0.6054\n",
            "Validation loss: 1.0347, Validation accuracy: 0.6406\n",
            "Saving best model for model_bn_relu.pth...\n",
            "\n",
            "Epoch 10:\n",
            "Training loss: 1.1198, Training accuracy: 0.6072\n",
            "Validation loss: 1.0637, Validation accuracy: 0.6330\n",
            "\n",
            "Epoch 11:\n",
            "Training loss: 1.1131, Training accuracy: 0.6125\n",
            "Validation loss: 1.0970, Validation accuracy: 0.6198\n",
            "\n",
            "Epoch 12:\n",
            "Training loss: 1.0938, Training accuracy: 0.6198\n",
            "Validation loss: 1.0502, Validation accuracy: 0.6336\n",
            "\n",
            "Epoch 13:\n",
            "Training loss: 1.0926, Training accuracy: 0.6236\n",
            "Validation loss: 1.0823, Validation accuracy: 0.6226\n",
            "\n",
            "Epoch 14:\n",
            "Training loss: 1.0841, Training accuracy: 0.6264\n",
            "Validation loss: 1.0108, Validation accuracy: 0.6504\n",
            "Saving best model for model_bn_relu.pth...\n",
            "\n",
            "Epoch 15:\n",
            "Training loss: 1.0739, Training accuracy: 0.6270\n",
            "Validation loss: 1.0050, Validation accuracy: 0.6512\n",
            "Saving best model for model_bn_relu.pth...\n",
            "\n",
            "Epoch 16:\n",
            "Training loss: 1.0704, Training accuracy: 0.6280\n",
            "Validation loss: 0.9967, Validation accuracy: 0.6578\n",
            "Saving best model for model_bn_relu.pth...\n",
            "\n",
            "Epoch 17:\n",
            "Training loss: 1.0556, Training accuracy: 0.6324\n",
            "Validation loss: 1.0257, Validation accuracy: 0.6458\n",
            "\n",
            "Epoch 18:\n",
            "Training loss: 1.0479, Training accuracy: 0.6368\n",
            "Validation loss: 0.9559, Validation accuracy: 0.6668\n",
            "Saving best model for model_bn_relu.pth...\n",
            "\n",
            "Epoch 19:\n",
            "Training loss: 1.0639, Training accuracy: 0.6320\n",
            "Validation loss: 1.0300, Validation accuracy: 0.6448\n",
            "\n",
            "Epoch 20:\n",
            "Training loss: 1.0451, Training accuracy: 0.6364\n",
            "Validation loss: 0.9740, Validation accuracy: 0.6620\n",
            "\n",
            "Epoch 21:\n",
            "Training loss: 1.0398, Training accuracy: 0.6385\n",
            "Validation loss: 1.0377, Validation accuracy: 0.6454\n",
            "\n",
            "Epoch 22:\n",
            "Training loss: 1.0281, Training accuracy: 0.6418\n",
            "Validation loss: 1.0387, Validation accuracy: 0.6450\n",
            "\n",
            "Epoch 23:\n",
            "Training loss: 1.0271, Training accuracy: 0.6442\n",
            "Validation loss: 0.9613, Validation accuracy: 0.6688\n",
            "Saving best model for model_bn_relu.pth...\n",
            "\n",
            "Epoch 24:\n",
            "Training loss: 1.0182, Training accuracy: 0.6496\n",
            "Validation loss: 0.9471, Validation accuracy: 0.6714\n",
            "Saving best model for model_bn_relu.pth...\n",
            "\n",
            "Epoch 25:\n",
            "Training loss: 1.0239, Training accuracy: 0.6466\n",
            "Validation loss: 0.9603, Validation accuracy: 0.6724\n",
            "Saving best model for model_bn_relu.pth...\n",
            "\n",
            "Epoch 26:\n",
            "Training loss: 1.0119, Training accuracy: 0.6481\n",
            "Validation loss: 0.9633, Validation accuracy: 0.6646\n",
            "\n",
            "Epoch 27:\n",
            "Training loss: 1.0101, Training accuracy: 0.6503\n",
            "Validation loss: 0.9331, Validation accuracy: 0.6746\n",
            "Saving best model for model_bn_relu.pth...\n",
            "\n",
            "Epoch 28:\n",
            "Training loss: 1.0040, Training accuracy: 0.6552\n",
            "Validation loss: 0.9063, Validation accuracy: 0.6828\n",
            "Saving best model for model_bn_relu.pth...\n",
            "\n",
            "Epoch 29:\n",
            "Training loss: 1.0020, Training accuracy: 0.6540\n",
            "Validation loss: 0.9398, Validation accuracy: 0.6780\n",
            "\n",
            "==================================================\n",
            "==> Training finished for model_bn_relu.pth! Best validation accuracy: 0.6828\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train SimpleNN with BN & Swish\n",
        "model_bn_swish = SimpleNN_BN_Swish().to(device)\n",
        "optimizer_bn_swish = torch.optim.SGD(model_bn_swish.parameters(), lr=0.1, momentum=0.9, weight_decay=1e-4)\n",
        "train_model(model_bn_swish, optimizer_bn_swish, \"model_bn_swish.pth\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-f08TnVQi0Y",
        "outputId": "06c30bf3-ffa5-434d-a085-7f08680f5a33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial loss before training: 2.3003, Initial accuracy: 0.1016\n",
            "==> Training model_bn_swish.pth model\n",
            "==================================================\n",
            "Epoch 0:\n",
            "Training loss: 1.6372, Training accuracy: 0.3922\n",
            "Validation loss: 1.3801, Validation accuracy: 0.4964\n",
            "Saving best model for model_bn_swish.pth...\n",
            "\n",
            "Epoch 1:\n",
            "Training loss: 1.3254, Training accuracy: 0.5226\n",
            "Validation loss: 1.2113, Validation accuracy: 0.5664\n",
            "Saving best model for model_bn_swish.pth...\n",
            "\n",
            "Epoch 2:\n",
            "Training loss: 1.2172, Training accuracy: 0.5679\n",
            "Validation loss: 1.0912, Validation accuracy: 0.6144\n",
            "Saving best model for model_bn_swish.pth...\n",
            "\n",
            "Epoch 3:\n",
            "Training loss: 1.1482, Training accuracy: 0.5909\n",
            "Validation loss: 1.0630, Validation accuracy: 0.6160\n",
            "Saving best model for model_bn_swish.pth...\n",
            "\n",
            "Epoch 4:\n",
            "Training loss: 1.1028, Training accuracy: 0.6109\n",
            "Validation loss: 1.0412, Validation accuracy: 0.6376\n",
            "Saving best model for model_bn_swish.pth...\n",
            "\n",
            "Epoch 5:\n",
            "Training loss: 1.0628, Training accuracy: 0.6261\n",
            "Validation loss: 1.0084, Validation accuracy: 0.6438\n",
            "Saving best model for model_bn_swish.pth...\n",
            "\n",
            "Epoch 6:\n",
            "Training loss: 1.0313, Training accuracy: 0.6363\n",
            "Validation loss: 0.9681, Validation accuracy: 0.6544\n",
            "Saving best model for model_bn_swish.pth...\n",
            "\n",
            "Epoch 7:\n",
            "Training loss: 1.0115, Training accuracy: 0.6437\n",
            "Validation loss: 0.9530, Validation accuracy: 0.6664\n",
            "Saving best model for model_bn_swish.pth...\n",
            "\n",
            "Epoch 8:\n",
            "Training loss: 0.9906, Training accuracy: 0.6501\n",
            "Validation loss: 0.9101, Validation accuracy: 0.6780\n",
            "Saving best model for model_bn_swish.pth...\n",
            "\n",
            "Epoch 9:\n",
            "Training loss: 0.9748, Training accuracy: 0.6584\n",
            "Validation loss: 0.9854, Validation accuracy: 0.6648\n",
            "\n",
            "Epoch 10:\n",
            "Training loss: 0.9551, Training accuracy: 0.6629\n",
            "Validation loss: 0.8789, Validation accuracy: 0.6906\n",
            "Saving best model for model_bn_swish.pth...\n",
            "\n",
            "Epoch 11:\n",
            "Training loss: 0.9528, Training accuracy: 0.6675\n",
            "Validation loss: 0.8993, Validation accuracy: 0.6892\n",
            "\n",
            "Epoch 12:\n",
            "Training loss: 0.9346, Training accuracy: 0.6716\n",
            "Validation loss: 0.8985, Validation accuracy: 0.6842\n",
            "\n",
            "Epoch 13:\n",
            "Training loss: 0.9312, Training accuracy: 0.6729\n",
            "Validation loss: 0.8555, Validation accuracy: 0.7110\n",
            "Saving best model for model_bn_swish.pth...\n",
            "\n",
            "Epoch 14:\n",
            "Training loss: 0.9171, Training accuracy: 0.6781\n",
            "Validation loss: 0.8705, Validation accuracy: 0.6908\n",
            "\n",
            "Epoch 15:\n",
            "Training loss: 0.9036, Training accuracy: 0.6844\n",
            "Validation loss: 0.9454, Validation accuracy: 0.6822\n",
            "\n",
            "Epoch 16:\n",
            "Training loss: 0.9027, Training accuracy: 0.6821\n",
            "Validation loss: 0.8425, Validation accuracy: 0.7046\n",
            "\n",
            "Epoch 17:\n",
            "Training loss: 0.8880, Training accuracy: 0.6890\n",
            "Validation loss: 0.9235, Validation accuracy: 0.6746\n",
            "\n",
            "Epoch 18:\n",
            "Training loss: 0.8824, Training accuracy: 0.6885\n",
            "Validation loss: 0.8204, Validation accuracy: 0.7206\n",
            "Saving best model for model_bn_swish.pth...\n",
            "\n",
            "Epoch 19:\n",
            "Training loss: 0.8876, Training accuracy: 0.6907\n",
            "Validation loss: 0.8568, Validation accuracy: 0.7080\n",
            "\n",
            "Epoch 20:\n",
            "Training loss: 0.8775, Training accuracy: 0.6946\n",
            "Validation loss: 0.8327, Validation accuracy: 0.7150\n",
            "\n",
            "Epoch 21:\n",
            "Training loss: 0.8698, Training accuracy: 0.6941\n",
            "Validation loss: 0.8405, Validation accuracy: 0.7088\n",
            "\n",
            "Epoch 22:\n",
            "Training loss: 0.8715, Training accuracy: 0.6960\n",
            "Validation loss: 0.8831, Validation accuracy: 0.6972\n",
            "\n",
            "Epoch 23:\n",
            "Training loss: 0.8754, Training accuracy: 0.6942\n",
            "Validation loss: 0.8408, Validation accuracy: 0.7114\n",
            "\n",
            "Epoch 24:\n",
            "Training loss: 0.8640, Training accuracy: 0.6995\n",
            "Validation loss: 0.8337, Validation accuracy: 0.7056\n",
            "\n",
            "Epoch 25:\n",
            "Training loss: 0.8558, Training accuracy: 0.6988\n",
            "Validation loss: 0.8037, Validation accuracy: 0.7202\n",
            "\n",
            "Epoch 26:\n",
            "Training loss: 0.8528, Training accuracy: 0.7028\n",
            "Validation loss: 0.8410, Validation accuracy: 0.7030\n",
            "\n",
            "Epoch 27:\n",
            "Training loss: 0.8516, Training accuracy: 0.7005\n",
            "Validation loss: 0.8784, Validation accuracy: 0.6944\n",
            "\n",
            "Epoch 28:\n",
            "Training loss: 0.8475, Training accuracy: 0.7051\n",
            "Validation loss: 0.7969, Validation accuracy: 0.7224\n",
            "Saving best model for model_bn_swish.pth...\n",
            "\n",
            "Epoch 29:\n",
            "Training loss: 0.8384, Training accuracy: 0.7067\n",
            "Validation loss: 0.7986, Validation accuracy: 0.7198\n",
            "\n",
            "==================================================\n",
            "==> Training finished for model_bn_swish.pth! Best validation accuracy: 0.7224\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Swish performs better than Relu as it has increased the validation accuracy from 68% to 72%"
      ],
      "metadata": {
        "id": "q7DQpIP2SpBC"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}